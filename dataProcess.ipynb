{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.matcher import Matcher\n",
    "# from spacy.matcher import PhraseMatcher\n",
    "# from spacy import displacy\n",
    "import re,string\n",
    "\n",
    "import docx\n",
    "import os\n",
    "import emoji\n",
    "\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from pyltp import SentenceSplitter,Segmentor,Postagger,NamedEntityRecognizer,Parser,SementicRoleLabeller\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = './data/solution/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件操作\n",
    "# 读取docx文件\n",
    "def readDocx(filePath):\n",
    "    fullText=[]\n",
    "    # fullText=\"\"\n",
    "    doc = docx.Document(filePath)\n",
    "    for p in doc.paragraphs:\n",
    "        fullText.append(p.text)\n",
    "        # fullText = fullText+(p.text)\n",
    "    return fullText\n",
    "\n",
    "# 读取所有文件\n",
    "def findAllFile(base):\n",
    "    for root,dirs,files in os.walk(base):\n",
    "        for f in files:\n",
    "            yield f\n",
    "\n",
    "# 写入csv中\n",
    "def writeCSV(data_ls):\n",
    "    df=pd.DataFrame(data_ls)\n",
    "    df.to_csv(csvPath,mode='a',index=False)\n",
    "\n",
    "# 写入docx\n",
    "def writeDocx(text,filePath):\n",
    "    doc = docx.Document()\n",
    "    p = doc.add_paragraph(text)\n",
    "    doc.save(filePath)\n",
    "\n",
    "\n",
    "# filePath_ls是solution文件夹下所有文件的文件名\n",
    "def getAllFilePath(filePath_ls):\n",
    "    for file in findAllFile(filePath):\n",
    "        filePath_ls.append(file)\n",
    "    # filePath_ls.sort(key=lambda x:int(x.split('.')[:-1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\n",
    "# 1、将表情包转为符号表示\n",
    "# 2、删除所有的英文，下划线，数字\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    char_pattern = re.compile(r'[\\n\\r\\t]')\n",
    "\n",
    "    return char_pattern.sub('',text)\n",
    "\n",
    "def emoji2code(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "\n",
    "# 3.删除表情符号\n",
    "def remove_emoji(text):\n",
    "    res = emoji.demojize(text)\n",
    "    try:  \n",
    "        co = re.compile(u'['u'\\U0001F300-\\U0001F64F' u'\\U0001F680-\\U0001F6FF'u'\\u2600-\\u2B55]+')  \n",
    "    except re.error:  \n",
    "        co = re.compile(u'('u'\\ud83c[\\udf00-\\udfff]|'u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'u'[\\u2600-\\u2B55])+')  \n",
    "    return co.sub('', res)\n",
    "\n",
    "\n",
    "# 删除markdown语法\n",
    "def remove_markdown(text):\n",
    "    markdown_pattern = re.compile(r'```.*?```|:.*?:|!\\[.*?\\)|[#*$\\\\\\|]|<!.*?>|\\[.*?\\)')\n",
    "    return markdown_pattern.sub('',text)\n",
    "\n",
    "# 删除标点符号\n",
    "def remove_punct(text):\n",
    "    punct = \"-：`+=(∩){},/;&.（）>!%\\\"'<\\[\\]:?“”、！①②③④⑤？—「」^~【】@Σ∣\"\n",
    "    return re.sub(r\"[%s]+\" %punct, \"\",text)\n",
    "\n",
    "# 删除字母数字下划线\n",
    "def remove_num_alpha(text):\n",
    "    pattern = re.compile(r'[A-Za-z0-9_]')\n",
    "    return pattern.sub('',text)\n",
    "\n",
    "# 删除空格\n",
    "def remove_whiteSpace(text):\n",
    "    return text.replace(\" \",\"\")\n",
    "\n",
    "#创建停用词表\n",
    "# def remove_stopwords(filepath,text):\n",
    "# \tstopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "# \tfor words in stopwords:\n",
    "#         if not \n",
    "#     # return stopwords\n",
    "\n",
    "\n",
    "\n",
    "# 分句，也就是将一片文本分割为独立的句子\n",
    "def sentence_splitter(sentence):\n",
    "\tsents = SentenceSplitter.split(sentence)  # 分句\n",
    "\treturn sents\n",
    "\t# print(sents)\n",
    "\t# print('\\n'.join(sents))\n",
    "\n",
    "\n",
    "def complete_noise(data):\n",
    "    new_data = remove_special_characters(data)\n",
    "    new_data = emoji2code(new_data)\n",
    "    new_data = remove_markdown(new_data)\n",
    "    \n",
    "    new_data = remove_num_alpha(new_data)\n",
    "    new_data = remove_punct(new_data)\n",
    "    new_data = remove_whiteSpace(new_data)\n",
    "    return new_data\n",
    "\n",
    "def create_text(list):\n",
    "    result_ls = []\n",
    "    for item in list:\n",
    "        result_ls.append(str(complete_noise(item)))\n",
    "    \n",
    "    result_str = ''.join(result_ls)\n",
    "    return result_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTP_DATA_DIR='D:\\LTPmodel\\ltp_data_v3.4.0'\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\t # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')\t# 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(sentence):\n",
    "\tsegmentor = Segmentor(cws_model_path,'D:\\zoe\\实验\\meProject\\Algorithm_KnowledgeGraph\\data\\lexicon.txt')\t # 初始化实例\n",
    "\t# segmentor.load(cws_model_path)\t# 加载模型\n",
    "\t#segmentor.load_with_lexicon('cws_model_path', 'D:\\pyprojects\\LTP\\ltp_data\\dict.txt') #加载模型\t  使用用户自定义字典的高级分词\n",
    "\twords = segmentor.segment(sentence)\t # 分词\n",
    "\t# 默认可以这样输出\n",
    "\t\n",
    "\t# 可以转换成List 输出\n",
    "\t# words_list = list(words)\n",
    "\tsegmentor.release()\t # 释放模型\n",
    "\treturn words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = readDocx('./data/solution/2.两数相加.docx')\n",
    "data=create_text(list)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分句\n",
    "sent = sentence_splitter(data)\n",
    "for s in sent:\n",
    "    print(s,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['此外', '，', '如果', '链表', '遍历', '结束', '后', '，', '有', '，', '还', '需要', '在', '答案', '链表', '的', '后面', '附加', '一个', '节点', '，', '节点', '的', '值', '为', '。']\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "seg = segmentor(sent[7])\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建停用词表\n",
    "def remove_stopwords_punct(text,path):\n",
    "\tresult_ls = []\n",
    "\tstopwords = [line.strip() for line in open(path, 'r', encoding='utf-8').readlines()]\n",
    "\tfor word in text:\n",
    "\t\tif word not in stopwords:\n",
    "\t\t\tresult_ls.append(str(word))\n",
    "\tresult_str = ''.join(result_ls)\n",
    "\treturn result_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoppath = 'D:\\zoe\\实验\\meProject\\Algorithm_KnowledgeGraph\\stopwords\\ltp_stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "链表遍历结束后还需要答案链表后面附加节点节点值\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords_punct(seg,stoppath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sent:\n",
    "    seg = segmentor(s)\n",
    "    seg = remove_stopwords_punct(seg,stoppath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 write successfully!\n"
     ]
    }
   ],
   "source": [
    "filePath_ls=[]\n",
    "getAllFilePath(filePath_ls)\n",
    "for index in range(1,2):\n",
    "    path = filePath+filePath_ls[index]\n",
    "    list=readDocx(path)\n",
    "    doc = docx.Document()\n",
    "    data=create_text(list)\n",
    "    sent = sentence_splitter(data)\n",
    "    for s in sent:\n",
    "        seg = segmentor(s)\n",
    "        seg = remove_stopwords_punct(seg,stoppath)\n",
    "        p = doc.add_paragraph(seg)\n",
    "    doc.save(path)\n",
    "    print(index,'write successfully!')\n",
    "# for file in filePath_ls:\n",
    "#     path=filePath+file\n",
    "#     text=readDocx(path)\n",
    "#     ls=[]\n",
    "#     for item in text:\n",
    "#         item=remove_special_characters(item)\n",
    "#         item=remove_stop_words_and_punct(item)\n",
    "#         matche(ls,item,dic_ls)\n",
    "#     ls=remove_duplicates(ls)\n",
    "#     data_ls.append(ls)\n",
    "        \n",
    "#     print(file+\"文件匹配完毕！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
