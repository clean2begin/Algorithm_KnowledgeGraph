{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import docx\n",
    "# 可视化\n",
    "from graphviz import Digraph\n",
    "from pyltp import SentenceSplitter,Segmentor,Postagger,NamedEntityRecognizer,Parser,SementicRoleLabeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = './data/operation.csv'\n",
    "filePath='./data/solutionRes/'\n",
    "relationPath='./data/relation.csv'\n",
    "\n",
    "LTP_DATA_DIR='D:\\LTPmodel\\ltp_data_v3.4.0'\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')\t # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')\t# 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文件操作\n",
    "def readDocx(filePath):\n",
    "    fullText=[]\n",
    "    # fullText=\"\"\n",
    "    doc = docx.Document(filePath)\n",
    "    for p in doc.paragraphs:\n",
    "        fullText.append(p.text)\n",
    "        # fullText = fullText+(p.text)\n",
    "    return fullText\n",
    "\n",
    "\n",
    "# 读取所有文件\n",
    "def findAllFile(base):\n",
    "    for root,dirs,files in os.walk(base):\n",
    "        for f in files:\n",
    "            yield f\n",
    "\n",
    "# 写入csv中\n",
    "def writeCSV(data_ls):\n",
    "    df=pd.DataFrame(data_ls)\n",
    "    df.to_csv(csvPath,mode='a',index=False)\n",
    "\n",
    "# filePath_ls是solution文件夹下所有文件的文件名\n",
    "def getAllFilePath(filePath_ls):\n",
    "\n",
    "    for file in findAllFile(filePath):\n",
    "        filePath_ls.append(file)\n",
    "    # filePath_ls.sort(key=lambda x:int(x.split('.')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zwm11\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.866 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 分词\n",
    "jieba.load_userdict('D:\\zoe\\实验\\meProject\\Algorithm_KnowledgeGraph\\dic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba词性标注\n",
    "def posttagger(sent):\n",
    "    words=pseg.cut(sent)\n",
    "    words_list = []\n",
    "    tags_list = []\n",
    "    for word,tag in words:\n",
    "        words_list.append(word)\n",
    "        tags_list.append(tag)\n",
    "        # print(word,tag)\n",
    "    return words_list,tags_list\n",
    "    # return tags_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将源文件与字典匹配，保留包含字典中的值的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典匹配\n",
    "# data_ls:包含标签的句子\n",
    "# tags_ls:通过模糊匹配得到的标签\n",
    "def match(sent,dic,data_ls,tags_ls):\n",
    "    matchname = process.extractOne(sent,dic_list,scorer=fuzz.partial_ratio)\n",
    "    if matchname[1] >= 60:\n",
    "        data_ls.append(sent)\n",
    "        tags_ls.append(matchname[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除重复词\n",
    "def remove_duplicates(text_ls):\n",
    "\n",
    "    check_val=set()\n",
    "    result=[]\n",
    "    for item in text_ls:\n",
    "        if item not in check_val:\n",
    "            result.append(item)\n",
    "            check_val.add(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_parser(words,tags):\n",
    "    parser = Parser(par_model_path)\n",
    "    arcs = parser.parse(words,tags)\n",
    "    parser.release()\n",
    "    return arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def extractOne(words,relation,heads):\n",
    "    relation_dic = ['VOB','SBV']\n",
    "    relation_ls = []\n",
    "   \n",
    "    for i in range(len(words)):\n",
    "        rel = []\n",
    "        if words[i] in tags_ls and relation[i] in relation_dic:\n",
    "            rel.append(words[i])\n",
    "            rel.append(heads[i])\n",
    "            relation_ls.append(rel)\n",
    "    return relation_ls\n",
    "    # writeCSV(relation_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRelation(arcs,words):\n",
    "    rely_id = [arc[0] for arc in arcs]  # 提取依存父节点id\n",
    "    relation = [arc[1] for arc in arcs]  # 提取依存关系\n",
    "    heads = ['Root' if id == 0 else words[id-1] for id in rely_id]  # 匹配依存父节点词语\n",
    "    return relation,heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "def visual(relation,words,heads):\n",
    "    g = Digraph('测试图片')\n",
    "\n",
    "    g.node(name='Root')\n",
    "    for word in words:\n",
    "        # print(word)\n",
    "        g.node(name=word,fontname='FangSong')\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        if relation[i] not in ['HED']:\n",
    "            g.edge(words[i], heads[i], label=relation[i])\n",
    "        else:\n",
    "            if heads[i] == 'Root':\n",
    "                g.edge(words[i], 'Root', label=relation[i])\n",
    "            else:\n",
    "                g.edge(heads[i], 'Root', label=relation[i])\n",
    "\n",
    "    g.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "words:分词结果\n",
    "tags:词性标注\n",
    "arcs:依存句法\n",
    "'''\n",
    "def extract(data_ls):\n",
    "    relation_dic = ['VOB','SBV']\n",
    "    relation_ls = []\n",
    "\n",
    "    # relation_ls.append(name)\n",
    "    for sentence in data_ls:\n",
    "        words = []\n",
    "        tags = []\n",
    "        relation = []\n",
    "        heads = []\n",
    "        words,tags=posttagger(sentence)\n",
    "        # print(sentence)\n",
    "        arcs = e_parser(words,tags)\n",
    "        \n",
    "        relation,heads=extractRelation(arcs,words)\n",
    "        # for r,w,h in zip(relation,words,heads):\n",
    "        #     print(r,w,h)\n",
    "        # visual(relation,words,heads)\n",
    "        # break\n",
    "        # relation_ls.append(extractOne(words,relation,heads))\n",
    "        # writeCSV(relation_ls)\n",
    "        for i in range(len(words)):\n",
    "            rel = []\n",
    "            if words[i] in tags_ls and relation[i] in relation_dic:\n",
    "                rel.append(words[i])\n",
    "                rel.append(heads[i])\n",
    "                relation_ls.append(rel)\n",
    "    # result_str = ''.join(relation_ls)\n",
    "\n",
    "    # print(result_str)\n",
    "    # with open (csvPath,'a',encoding='utf_8') as fp:\n",
    "    #     fp.write(result_str)\n",
    "    #     fp.close()\n",
    "\n",
    "    # print(relation_ls)\n",
    "    return relation_ls\n",
    "    \n",
    "    # writeCSV(relation_ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open('./lexicon.txt','r',encoding='utf-8')\n",
    "dic_list = []\n",
    "line = fp.readline().strip()\n",
    "dic_list.append(line)\n",
    "while line:\n",
    "    line = fp.readline().strip()\n",
    "    if line != '':\n",
    "        dic_list.append(line)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath_ls = []\n",
    "getAllFilePath(filePath_ls)\n",
    "df = pd.DataFrame(columns=['name','operations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# data_ls = []\n",
    "# tags_ls = []\n",
    "# for text in text_list:\n",
    "#     match(text,dic_list,data_ls,tags_ls)\n",
    "# # for data in data_ls:\n",
    "# #     print(data)\n",
    "# tags_ls = remove_duplicates(tags_ls)\n",
    "# for t in tags_ls:\n",
    "#     print(t)\n",
    "for idx in range(2031,2032):\n",
    "    path = filePath+filePath_ls[idx]\n",
    "    with open(path,'r',encoding='utf-8') as fp:\n",
    "        text_list = fp.readlines()\n",
    "        fp.close()    \n",
    "    data_ls = []\n",
    "    tags_ls = []\n",
    "    name = filePath_ls[idx].rsplit('.',1)[0]\n",
    "    for text in text_list:\n",
    "        match(text,dic_list,data_ls,tags_ls)\n",
    "    tags_ls = remove_duplicates(tags_ls)\n",
    "    relation_ls = extract(data_ls)\n",
    "    # extract(data_ls,filePath_ls[idx])\n",
    "\n",
    "    df.loc[idx]=[name,relation_ls]\n",
    "    print(idx,'successfully!')\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/operation2.csv',mode='a',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
